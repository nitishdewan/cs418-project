{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append( os.path.join(\"..\"))\n",
    "from UtilityFunctions import CommonHelpers,PreprocessHelpers,FeatureEngineering\n",
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "import spacy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sharath/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sharath/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sharath/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sharath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "name_pickle_path = \"../data/preprocessed/names_english.pickle\"\n",
    "book_names =  CommonHelpers.load_pickle(name_pickle_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_number_id(file_name):\n",
    "    regex = r\"([a-z_A-Z]+)(\\d+)\"\n",
    "    match = re.search(regex, file_name)\n",
    "    if match!=None:\n",
    "        return match.group(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df =pd.read_csv(\"../data/samples_80_limit.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df[\"category\"] = df[\"LoCC\"].str[:1]\n",
    "category_list_train = [int(get_number_id(each)) for each in book_names]\n",
    "selected_books = df[df[\"Text#\"].isin(category_list_train)]\n",
    "categories=[]\n",
    "titles=[]\n",
    "for each in category_list_train:\n",
    "    categories.append(selected_books[selected_books[\"Text#\"]==each][\"category\"].values[0])\n",
    "    titles.append(selected_books[selected_books[\"Text#\"]==each][\"Title\"].values[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pt = df.groupby([\"category\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ix = np.hstack([np.random.choice(v, 5, replace=False) for v in pt.groups.values()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print(ix.tolist())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[65, 18, 51, 59, 20, 131, 88, 125, 141, 87, 150, 194, 195, 182, 202, 287, 272, 254, 265, 246, 338, 340, 334, 316, 344, 429, 394, 368, 411, 397, 465, 482, 467, 468, 466, 512, 549, 572, 557, 526, 655, 627, 617, 600, 596, 729, 709, 711, 718, 716, 768, 785, 745, 740, 747, 841, 819, 808, 810, 814, 903, 884, 882, 918, 931, 1004, 1017, 971, 953, 954, 1081, 1089, 1035, 1046, 1032, 1144, 1126, 1099, 1108, 1098, 1194, 1238, 1183, 1178, 1184, 1243, 1259, 1289, 1279, 1284, 1353, 1369, 1316, 1350, 1314, 1448, 1399, 1418, 1459, 1423, 1480, 1468, 1502, 1509, 1525]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "l1 = df.iloc[ix.tolist()]\n",
    "l1=l1.reset_index()\n",
    "l2=l1[\"Text#\"].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "RAW_BOOK_FOLDER = os.path.join(\"..\",\"data_collection\",\"raw_books\")\n",
    "\n",
    "file_list = [\"raw_book\"+str(i)+\".pickle\" for i in l2]\n",
    "full_text= CommonHelpers.load_pickle(os.path.abspath(os.path.join(RAW_BOOK_FOLDER, file_list[0])))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_entity(doc,label=\"PERSON\"):\n",
    "    ents = [e.text for e in doc.ents if e.label_==label]\n",
    "    return ents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ents=[]\n",
    "for each in file_list[:10]:\n",
    "    full_text= CommonHelpers.load_pickle(os.path.abspath(os.path.join(RAW_BOOK_FOLDER, each)))\n",
    "    ents.append(get_entity(nlp(full_text)))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1100191 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ca0bdb6490fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfull_text\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mCommonHelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_BOOK_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fall21/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \"\"\"\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fall21/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fall21/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \"\"\"\n\u001b[1;32m   1080\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1100191 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "len(ents)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "  \n",
    "frequency = collections.Counter(ents[5])\n",
    "print(frequency)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'Shakespeare': 11, 'Talmage': 9, 'Thomas Paine': 8, 'Robert G. Ingersoll': 8, 'Darwin': 5, 'Black': 5, 'Field': 5, 'Law': 4, 'Calvin': 4, 'Bruno': 4, 'Kepler': 4, 'Dorsey': 4, 'ROBERT G. INGERSOLL': 3, 'Farmer': 3, 'Knox': 3, 'SHAKESPEARE': 3, 'THE HOLY BIBLE': 3, 'Letter': 3, 'Jehovah': 3, 'Ananias': 3, 'Gladstone': 3, 'Gospels': 3, 'Buckley': 3, 'THOMAS PAINE': 3, 'Matthew Hale': 2, 'Abraham Lincoln': 2, 'Jonathan Edwards': 2, 'Gospel': 2, 'Aaron': 2, 'Zeno': 2, 'Virtues': 2, 'Henry Ward Beecher': 2, 'Heathen': 2, 'ROBERT BURNS': 2, 'Horace Greeley': 2, 'David Hume': 2, 'Mohammed': 2, 'INGERSOLL': 2, 'George Eliot': 2, 'Paine': 2, 'Manley': 2, 'Mary Hinsdale': 2, 'James Cheetham': 2, 'Policeman': 2, 'NORMILE': 2, 'JOHN HALL': 2, 'Robert Collyer': 2, 'Wright': 2, 'HENRY GEORGE': 2, 'Samuel J.\\nTilden': 2, 'Greenback': 2, 'ARGUMENT': 2, 'Conspiracy': 2, 'John W. Dorsey': 2, 'Stephen W. Dorsey': 2, 'John R. Miner': 2, 'Ker': 2, 'Davis': 2, 'C. B. REYNOLDS': 2, 'ERNEST RENAN': 2, 'Tolstoy': 2, 'Story': 2, 'ALEXANDER CLARK': 2, 'Cook': 2, 'David Widger': 1, 'Thomas Paine-A': 1, 'C. B. Reynolds': 1, 'THOMAS': 1, 'Human Slavery': 1, 'Intelligent Powers': 1, 'Evil Spirits—Origin of': 1, 'William Hamilton': 1, 'Gods': 1, 'Honoring Humboldt': 1, 'Franklin': 1, 'Thomas Aikenhead': 1, 'McKnights': 1, 'Corrupting': 1, 'the Black Flag': 1, \"John Calvin's\": 1, 'Michael Servetus': 1, 'John Wesley': 1, \"William Blackstone's\": 1, 'Timothy Dwight': 1, 'Genius': 1, 'Heva': 1, 'Beggars': 1, 'John Calvin': 1, 'Dimpled': 1, 'Belief': 1, 'Believe': 1, 'Athanasian Creed': 1, 'Episcopal Creed': 1, 'Wesley': 1, 'Whitfield': 1, 'John\\nCalvin': 1, \"Jeremy Taylor's\": 1, 'Free Schools': 1, 'the Inspired Writer—VIII': 1, 'Moon Manufactured': 1, 'Henry Morey': 1, \"Adam Clark's\": 1, 'Scott': 1, \"Matthew Henry's\": 1, 'Adam Clark': 1, 'Henry Dissents': 1, 'Clark': 1, 'Henry': 1, 'Noah': 1, 'Plagues': 1, 'Local': 1, 'Manna': 1, \"Alexander Cruden's\": 1, 'Devil': 1, 'Reputable Heathen': 1, 'Charles\\nDarwin': 1, 'Hanged\"—\"No Bible': 1, 'I. Happiness': 1, 'Beethoven': 1, 'Religions Naturally Produced': 1, 'Buddha': 1, 'Joss House': 1, 'Plays': 1, 'Intellectual Spendthrift': 1, 'X. Types': 1, 'John Barleycorn': 1, 'John Anderson': 1, 'Burns': 1, 'Louis Napoleon': 1, 'Edward Everett—Apologetic': 1, 'Lisbon': 1, 'Jean Calas': 1, 'Barre': 1, 'Walt Whitman': 1, 'Wreaths': 1, 'Archbishop Corrigan': 1, 'Miraculous Cures': 1, 'an Inspired Book Ought': 1, 'Vermont Farmer': 1, 'Robert Burns': 1, 'Keats': 1, 'Volney': 1, 'Intellectually Hospitable': 1, 'Brigham Young': 1, 'Whom': 1, 'Rag': 1, 'Matthew and Luke': 1, 'Superstition?—Popular Beliefs': 1, 'Significance': 1, 'Earthquakes': 1, 'Suffer a Witch': 1, 'the Brain Softens': 1, 'X. Possibly': 1, 'Mary Magdalene': 1, 'Bloody': 1, 'Cruel Laws': 1, 'Ingenuous Remark': 1, 'Barbarous Punishments': 1, 'Geography': 1, 'Free Bodies': 1, 'I. Belief': 1, 'My Creed': 1, 'Belittle': 1, 'Guiteau': 1, 'Moon': 1, 'Whale': 1, 'Answered': 1, 'Christian Millions': 1, 'Thomas\\nPaine': 1, 'Voltaire Died': 1, 'Thomas Nixon': 1, 'Daniel Pelton': 1, 'B. F. Has-kin': 1, 'Gilbert Vale': 1, 'Philip Graves': 1, 'Willet Hicks': 1, 'A. C. Hankinson': 1, 'John Hogeboom': 1, 'J. Hilton': 1, 'Tames Cheetham': 1, 'Hedden': 1, 'Andrew A. Dean': 1, 'William\\nCarver,—The Statements': 1, 'Mary Roscoe': 1, 'Hindsdale Examined': 1, \"William Cobbett's\": 1, 'Thorbum': 1, 'J. D.\\nWickham': 1, 'Charles Hawley': 1, 'D.D.—W. H.\\nLadd': 1, 'Purdy': 1, 'John\\nFellows': 1, 'James Wilburn': 1, 'Walter Morton': 1, 'Herttell': 1, 'H. Margary': 1, 'Elihu Palmer': 1, 'Lovett': 1, 'Temperate Man': 1, 'Thomas': 1, 'Watson': 1, 'William Carver': 1, 'E. F. Hatfield': 1, 'J. W.\\n': 1, 'Bishop Fenwick': 1, 'Mary Roscoeand Mary Hins-': 1, 'A. W. Cornell': 1, 'Grant Thorburn': 1, 'James Parton': 1, 'William B. Barnes': 1, 'HENRY M. FIELD': 1, 'Brahma': 1, 'Gibson': 1, 'Pennsylvania Quoted': 1, 'Henry M. Field': 1, 'Charles Darwin': 1, 'Orthodox Sabbath': 1, 'Hindu Prayer': 1, 'Human Greatness': 1, 'Greatness': 1, 'Cardinal Manning': 1, 'St Peter': 1, 'Human Sins': 1, 'Pope': 1, 'Canon Law\\n': 1, 'S. W. Dike': 1, 'Henry C. Potter': 1, 'Questions Answered': 1, 'Bradley': 1, 'Abbott': 1, 'Roger Bacon': 1, 'Bancroft': 1, 'Compel Belief': 1, 'Guard': 1, 'Samuel Robinson': 1, 'Christian Advocate': 1, 'Universe': 1, 'Brahmins': 1, 'Kohler': 1, 'Ryder': 1, 'David Walk': 1, 'Ball': 1, 'T. B. Taylor': 1, 'William Thomson': 1, 'Eddy': 1, 'Hawkins': 1, 'Haynes': 1, 'Pullman': 1, 'Foote': 1, 'Wells': 1, 'Van Dyke': 1, 'Reed': 1, 'McClelland': 1, 'Baker': 1, 'Fransiola': 1, 'Frederic R. Coudert': 1, 'Stewart L. Woodford': 1, 'Persecute': 1, 'Intellectual Hospitality': 1, 'False Witness': 1, 'Coudert': 1, 'Ingersoll': 1, 'J. M. King': 1, 'Thomas Dixon': 1, 'Prophet': 1, 'Peters': 1, 'DaCosta—\"Human Brotherhood': 1, 'Browning': 1, 'Stephen Girard': 1, 'James Lick': 1, 'Hindoo': 1, 'John\\nEricsson': 1, 'Ballou': 1, 'Hillier': 1, 'Haldeman': 1, 'George A. Locey': 1, 'J. Benson Hamilton': 1, 'Holloway': 1, 'Tyler': 1, 'Charles Deems': 1, 'Roman Law': 1, 'Justifiable': 1, 'Alerle St. Croix': 1, \"Rush Hawkins'\": 1, 'Brakeman': 1, 'Mosaic Cosmogony': 1, 'Christian Threats': 1, 'Comegys—Wilmington\\nPreachers': 1, 'Whipping': 1, 'Bergh': 1, 'Hall': 1, 'Sayings Repetitions': 1, 'Mills': 1, 'Roberts': 1, 'MacArthur': 1, 'J. Lewis Parks': 1, 'E.\\nF. Moldehnke': 1, 'Belcher': 1, 'W. C. Buchanan': 1, 'J.\\nW. Campbell': 1, 'Henry Frank': 1, 'BEECHER': 1, 'GUITEAU': 1, 'JOHN G. MILLS AND': 1, 'WENDELL PHILLIPS': 1, 'B. F. MORSE': 1, 'JAMES G. BLAINE': 1, 'ROBERT ELSMERE': 1, 'WOMAN SUFFRAGE': 1, 'PLAYS': 1, 'CLEVELAND': 1, 'THE A. P. A.': 1, 'L. A. BANKS': 1, 'JOHN RUSSELL': 1, 'William Lloyd Garrison': 1, 'Wendell Phillips': 1, 'Charles Sumner': 1, 'John\\nBrown': 1, 'Andrew Jackson': 1, 'Grant': 1, 'Roger Williams': 1, 'Jacob Thompson': 1, 'the Dismantled Mill': 1, 'Congratulatory Letter': 1, 'Rutherford B. Hayes': 1, 'Implacables': 1, 'North—\"I': 1, 'Bankruptcy': 1, 'Fare': 1, 'Honor Pledged': 1, 'Path': 1, 'Suffrage': 1, 'Morey': 1, 'Patriotic': 1, 'Jacob Rehm': 1, 'Golsen Reviewed': 1, 'Rehm': 1, 'Based': 1, 'False Claims': 1, 'Stephen W.': 1, 'Corpus Delicti': 1, 'Guilt': 1, 'John M. Peck': 1, '(A. E. )': 1, 'Peck': 1, 'Vaile': 1, 'James W. Bosler': 1, 'Received': 1, 'Discontinued Routes': 1, 'Conspiracy?—Dorsey': 1, 'Rerdell': 1, 'Faster Time': 1, 'Merrick': 1, 'Miner': 1, 'Carey': 1, 'Frank A.\\n': 1, 'Bliss': 1, 'Mitchell': 1, 'Miles': 1, \"Tames W. Bosler's\": 1, 'Mistaken Regarding a Decision': 1, 'John\\nW. Miner': 1, 'Henkle': 1, 'Verdict': 1, 'Will': 1, 'John A. Davis': 1, 'Stand': 1, \"Mary Ann Davis's\": 1, 'Russell': 1, 'Frederick Douglass(\"Abou': 1, 'Wilson': 1, 'Obvious Purpose': 1, 'Miller': 1, 'Law?—The Word': 1, 'Harlan': 1, 'Taken Place': 1, 'Blasphemy Laws': 1, 'The Feudal System': 1, 'Remedy': 1, 'Discipline': 1, 'Classes': 1, 'Gould': 1, 'Purpose': 1, 'de Milo': 1, 'I. Inharmony': 1, 'Huxley': 1, 'Auguste Comte': 1, 'Harrison': 1, 'Blasphemy': 1, \"James Monroe's\": 1, 'A.': 1, 'B.': 1, 'C.': 1, 'Common Prey': 1, 'Kick': 1, \"H. Hodson Rugg's\": 1, 'Horizon': 1, 'Humanity': 1, 'Wife': 1, 'John Ward': 1, 'Newton': 1, 'Ours': 1, 'Relations': 1, 'John Hall': 1, 'Jew Knows': 1, 'Prelude': 1, 'Wild Beasts': 1, 'Asylums': 1, 'Lick Observatory': 1, 'Maybrick': 1, 'Depew': 1, 'Decline': 1, 'Herbert': 1, \"EDGAR C. BEALL'S\": 1, 'GEORGE JACOB HOLYOAKE': 1, 'BENJAMIN W. PARKER': 1, 'JOHN G. MILLS': 1, 'RICHARD H. WHITING': 1, 'MARY H. FISKE': 1, 'LAWRENCE BARRETT': 1, 'THOMAS SETON ROBERTSON': 1, 'ISAAC H. BAILEY': 1, 'Robert GIngersoll\\n\\n\\n\\n    ': 1, 'Laborers': 1, 'Cabin': 1, 'Stingy': 1, 'Whips': 1, 'Settle': 1, 'Honesty Tells': 1, 'Column': 1, 'Mule Equality\\n': 1, 'Epitaph': 1, 'Science Found': 1, 'Black People': 1, 'Voter': 1, 'Crime Rampant': 1, 'Corpse': 1, 'Bold Assertion': 1, 'Religion Demanding Miracles': 1, 'Donkey': 1, 'Absurd': 1, 'Foolish Fables': 1, 'Laugh': 1, 'Infidels Die': 1, 'Jefferson Die': 1, 'Benedict Spinoza': 1, 'Intellectual Hera': 1, 'John Calvin\\n': 1, 'J. G. Blaine\\n': 1, 'Jeremy Bentham': 1, 'Charles Fourier': 1, 'Herbert Spencer': 1, 'John Milton\\n347': 1, 'Ernst Haeckel\\n': 1, 'Dove': 1, 'Rabbi Bien': 1, 'Garfield': 1, 'W. Hiram': 1, 'Thomas\\nMISCELLANEOUS': 1, 'Survive': 1, 'The Sacred Sabbath\\n': 1, 'Blasphemer': 1, 'Joshua': 1, 'Duke Orang-Outang': 1, 'Conscience': 1, 'Jehovah Breaking His Own Laws\\n': 1, 'Promise Broken': 1, 'Character Bather': 1, 'Mohammed the Prophet': 1, 'Succeeds Inspired': 1, 'Zeno\\n460': 1, 'Whence Came the Gospels': 1, 'JOHN CHINAMAN\\n': 1, 'John Chinaman\\n': 1, 'Henry VIII': 1, 'Spencer': 1, 'Darwin Damned': 1, 'Names Belittle': 1})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('fall21': conda)"
  },
  "interpreter": {
   "hash": "e3f678557ebf47878347fa9a6f9a07da5ceab10869ea353b8e2957e3132370f4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}